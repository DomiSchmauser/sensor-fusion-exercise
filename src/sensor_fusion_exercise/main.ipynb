{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67f8a368-34db-4e38-a552-9d67e0514782",
   "metadata": {},
   "source": [
    "# Sensor Fusion Exercise\n",
    "Welcome to the EDTH sensor fusion exercise in which you will:\n",
    "- Set up an array of two camera sensors that are 50 meters apart from each other and observe a 2D map (flat surface)\n",
    "- Derive an object recognition algorithm to detect and localise a tank and a car in the map\n",
    "- Implement the core logic for fusing identified objects into a common operational picture\n",
    "\n",
    "### Setting\n",
    "We use a local right-handed cartesian coordinate system for which `x` points north and `y` points east. Each camera is facing towards the objects which is configured by the\n",
    "cameras bearing (or azimuth) angle with 0° bearing = facing parallel north and 90° bearing = facing parallel east.\n",
    "\n",
    "<img src='resources/setting.png' width=25% height=25%/>\n",
    "\n",
    "### Modules\n",
    "This exercise comprises of 3 modules:\n",
    "- `sensor.py` defining the camera logic for streaming images\n",
    "- `object_recognition.py` defining the algorithm to process camera images, detect objects and generate a unique object identification\n",
    "- `fusion_station.py` defining the central system to fuse identified objects into a shared representation\n",
    "\n",
    "### Setup\n",
    "Install required packages via [poetry](https://python-poetry.org/docs/) by executing from the source directory:\n",
    "```\n",
    "# Ensure Python >= 3.10 is installed, otherwise you can download with\n",
    "pyenv install 3.10\n",
    "pyenv global 3.10\n",
    "\n",
    "# Install the virtual environment\n",
    "poetry shell\n",
    "poetry install\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d14a1ddfaadd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from sensor_fusion_exercise.fusion_station import FusionStation\n",
    "from sensor_fusion_exercise.object_recognition import BoundingBox, IdentifiedObject, MockObjectRecognition\n",
    "from sensor_fusion_exercise.sensor import CameraConfig, Frame, MockCamera, Sensor\n",
    "from sensor_fusion_exercise.utils import OBJECT_PRIORS_WIDTH, LocationNE, Meters, SensorId\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfec4db503653829",
   "metadata": {},
   "source": [
    "### 1. Sensor array\n",
    "First, we place a sensor array in the map which sets 2 cameras that are 50 meters apart to each other w.r.t. east. Each one is facing a tank and car such that both are visible in the image as depicted in the drawing above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8c87c3e71695eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_cfg = CameraConfig(image_width=1920, image_height=1080, fov_horizontal=40.0, fov_vertical=22.5)\n",
    "\n",
    "sensor_1 = MockCamera(\n",
    "    config=camera_cfg, asset_location_ne=LocationNE(north=0.0, east=0.0), bearing_angle=26.6, sensor_id=1\n",
    ")\n",
    "sensor_2 = MockCamera(\n",
    "    config=camera_cfg, asset_location_ne=LocationNE(north=0.0, east=50.0), bearing_angle=0.0, sensor_id=2\n",
    ")\n",
    "\n",
    "sensor_array: list[Sensor[Frame]] = [sensor_1, sensor_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad2e9ce74f57cd",
   "metadata": {},
   "source": [
    "### 2. Object detection and localisation from a single view\n",
    "Next, we initialise an object recognition algorithm by providing ground truth detections for the classes tank and car.\n",
    "The detection yields a bounding box in pixel coordinates and a class name.\n",
    "Given the bounding box size and the identified class, together with a known object size prior of the objects real width, we can estimate its distance from the camera.\n",
    "\n",
    "**Note**: This is just one of many possible approaches to localise an object from image coordinates in a map. One could also use triangulation from a stereo view from 2 or more cameras to get the object distances or use monocular depth estimation neural networks.\n",
    "\n",
    "**TODO**: Fill the missing logic of the distance estimation in the function `monocular_distance_from_object_prior` below.\n",
    "**Hint**: Trigonometry is your friend for solving this part.\n",
    "\n",
    "<img src='resources/mono_depth.png' width=25% height=25%/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3010ba56e6e2b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_objects = {\n",
    "    1: [\n",
    "        IdentifiedObject(\n",
    "            class_name=\"tank\",\n",
    "            bounding_box=BoundingBox(880, 500, 180, 100),\n",
    "            object_location_north_east=LocationNE.null(),\n",
    "        ),\n",
    "        IdentifiedObject(\n",
    "            class_name=\"car\", bounding_box=BoundingBox(800, 500, 60, 100), object_location_north_east=LocationNE.null()\n",
    "        ),\n",
    "    ],\n",
    "    2: [\n",
    "        IdentifiedObject(\n",
    "            class_name=\"car\", bounding_box=BoundingBox(1200, 500, 70, 100), object_location_north_east=LocationNE.null()\n",
    "        ),\n",
    "        IdentifiedObject(\n",
    "            class_name=\"tank\",\n",
    "            bounding_box=BoundingBox(860, 500, 180, 100),\n",
    "            object_location_north_east=LocationNE.null(),\n",
    "        ),\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def monocular_distance_from_object_prior(identified_object: IdentifiedObject, frame: Frame) -> Meters:\n",
    "    \"\"\"\n",
    "    Calculates the distance based on an object size prior and the respective bounding box size.\n",
    "    Returns a distance estimate from the camera to the target object in meters.\n",
    "    \"\"\"\n",
    "    ...\n",
    "    real_object_width = OBJECT_PRIORS_WIDTH[identified_object.class_name]\n",
    "    # ------------------ TODO: Fill the missing code, ~10min\n",
    "    #\n",
    "    #\n",
    "    # ------------------\n",
    "\n",
    "\n",
    "object_recognition = MockObjectRecognition(\n",
    "    ground_truth_objects=ground_truth_objects, monocular_distance_estimation=monocular_distance_from_object_prior\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378e45ab506f387",
   "metadata": {},
   "source": [
    "### 3. Sensor Fusion\n",
    "Finally, after having identified and localised the objects in each sensor we can fuse the multi-sensor representation into a single common operational picture.\n",
    "This is done by matching identified objects if they are similar.\n",
    "Defining the heuristic for similarity is up to the engineer (or the neural network).\n",
    "For this exercise we define a distance metric and generate a cost matrix for the assignment problem.\n",
    "\n",
    "First, let's visualise the common operational picture if we don't fuse identified objects and print the sensor object mapping from the object recognition algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff6ea2eebb3a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_and_fuse_identified_objects(\n",
    "    _sensor_object_mapping: dict[SensorId, list[IdentifiedObject]],\n",
    ") -> list[IdentifiedObject]:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "fusion_station = FusionStation(sensor_array, object_recognition, match_and_fuse_identified_objects)\n",
    "sensor_object_mapping = fusion_station.execute_without_fusion()\n",
    "\n",
    "logger.info(\"Sensor object mapping: \\n\")\n",
    "for items in sensor_object_mapping.items():\n",
    "    logger.info(f\"Sensor ID: {items[0]} with identified objects: {items[1]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b268d9ae287e291c",
   "metadata": {},
   "source": [
    "**TODO**: Next, let's fuse the identified objects into a common representation and visualise the common operational picture again. As a simplification for this exercise we know that both car and tank\n",
    "can be seen by each camera and hence, a 2-to-2 assignment exists.\n",
    "\n",
    "**Hint**: you can use [scipy's bipartite-graph matching](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_assignment.html) after defining\n",
    "a 2x2 cost matrix to solve the assignment problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dccc467e23702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_and_fuse_identified_objects(\n",
    "    sensor_object_mapping: dict[SensorId, list[IdentifiedObject]],\n",
    ") -> list[IdentifiedObject]:\n",
    "    \"\"\"\n",
    "    Matches identified objects of first sensor against the identified objects of the second sensor.\n",
    "    First, a distance metric is defined to derive the similarity/dissimilarity of object pairs.\n",
    "    Based on the distance metrics, one can find an optimal assignment between object pairs to\n",
    "    output a list of fused identified objects.\n",
    "    \"\"\"\n",
    "\n",
    "    def distance_metric(object1: IdentifiedObject, object2: IdentifiedObject) -> Any: ...\n",
    "\n",
    "    ...\n",
    "    # ------------------ TODO: Fill the missing code, ~20min\n",
    "    #\n",
    "    #\n",
    "    # ------------------\n",
    "\n",
    "\n",
    "fusion_station = FusionStation(sensor_array, object_recognition, match_and_fuse_identified_objects)\n",
    "fusion_station.execute_with_fusion()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
